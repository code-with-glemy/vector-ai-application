{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa1d3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import langchain_pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a04393",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f3e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataList=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44be319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd6268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "List=text_splitter.split_documents(DataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c564ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List[13].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e2872e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b616d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gleme\\AppData\\Local\\Temp\\ipykernel_10068\\2861471853.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "EmbededData=embeddings.embed_query(\"Hello world\")\n",
    "len(EmbededData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21cf221",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31c3d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f6a5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC=pinecone.Pinecone(\n",
    "    api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce5289fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.db_data.index.Index at 0x1c7647acd40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC.Index(\"pinecone-testing-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb89d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch=langchain_pinecone.Pinecone.from_texts(\n",
    "    [doc.page_content for doc in List],embedding=embeddings,\n",
    "    index_name=\"pinecone-testing-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32738040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_pinecone.vectorstores.Pinecone object at 0x000001C7647AF9B0>\n"
     ]
    }
   ],
   "source": [
    "print(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b683faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='304dbf0b-9cd8-4388-906a-ab739afeee66', metadata={}, page_content='be superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the'),\n",
       " Document(id='3a71b0e3-0443-4fbb-a28d-9a6488708cf8', metadata={}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(id='148c67db-cc8b-43af-b74d-289bb3de7b4f', metadata={}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of'),\n",
       " Document(id='e471a079-340f-4358-8718-82585b320716', metadata={}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.similarity_search(\"YOLOv7 outperforms which models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b731a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab582954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gleme\\AppData\\Local\\Temp\\ipykernel_10068\\631595585.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "llm=ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    temperature=0.9,\n",
    "    model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f8d42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce65a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gleme\\AppData\\Local\\Temp\\ipykernel_10068\\3507390869.py:1: RuntimeWarning: coroutine 'Chain.arun' was never awaited\n",
      "  y=qa.run(\"What is the difference between YOLOv7 and YOLOv8?\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "y=qa.run(\"What is the difference between YOLOv7 and YOLOv8?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69020e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have information on YOLOv7 or YOLOv8 in the provided context. If you have specific details or references related to these versions, I can try to help further.\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a741cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bf771",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:    \n",
    "    user_input = input(\"Enter a question (or 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the program.\")\n",
    "        sys.exit()\n",
    "    if user_input.strip() == \"\":\n",
    "        continue\n",
    "    result= qa({'query': user_input})\n",
    "    print(f\"Answer: {result['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
